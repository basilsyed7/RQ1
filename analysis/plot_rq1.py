#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Create figures comparing models across attributes and metrics.

Inputs (generated by analysis/analyze_rq1.py):
- analysis/outputs/summary_per_attribute.csv
- analysis/outputs/tests_all_metrics.csv (optional but recommended)

Outputs:
- analysis/figures/summary_<dimension>_<metric>.png
- analysis/figures/delta_<dimension>_<metric>.png

Usage examples:
  python3 analysis/plot_rq1.py
  python3 analysis/plot_rq1.py --metrics bias_score favorability refusal
"""
from pathlib import Path
import argparse
import math

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

try:
    import seaborn as sns  # optional; improves style
    sns.set_context("talk")
    sns.set_style("whitegrid")
except Exception:
    sns = None


ROOT = Path(__file__).resolve().parent.parent
OUT_DIR = ROOT / "analysis" / "figures"
OUT_DIR.mkdir(parents=True, exist_ok=True)


def _sig_stars(p: float) -> str:
    if pd.isna(p):
        return ""
    if p < 0.001:
        return "***"
    if p < 0.01:
        return "**"
    if p < 0.05:
        return "*"
    return ""


def plot_summary(df_sum: pd.DataFrame, dimension: str, metric: str, outdir: Path):
    data = df_sum[(df_sum["dimension"] == dimension) & (df_sum["metric"] == metric)].copy()
    if data.empty:
        return None

    # Sort attributes for consistent layout
    attrs = sorted(data["attribute_value"].unique())
    models = sorted(data["model"].unique())

    # Optional scaling for certain metrics
    as_percent = metric in {"refusal"}
    if as_percent:
        # convert to percentage
        for col in ["mean", "ci_lo", "ci_hi"]:
            data[col] = 100.0 * data[col]

    # X positions
    x = np.arange(len(attrs))
    n_models = len(models)
    width = 0.8 / max(1, n_models)
    offsets = (np.arange(n_models) - (n_models - 1) / 2.0) * width

    fig, ax = plt.subplots(figsize=(max(8, 1.2 * len(attrs)), 5.5))

    for m_idx, model in enumerate(models):
        sub = data[data["model"] == model].set_index("attribute_value")
        means = [sub.loc[a, "mean"] if a in sub.index else np.nan for a in attrs]
        lo = [sub.loc[a, "mean"] - sub.loc[a, "ci_lo"] if a in sub.index else np.nan for a in attrs]
        hi = [sub.loc[a, "ci_hi"] - sub.loc[a, "mean"] if a in sub.index else np.nan for a in attrs]
        ax.errorbar(x + offsets[m_idx], means, yerr=[lo, hi], fmt="o", capsize=4, label=model)

    ax.set_xticks(x)
    ax.set_xticklabels(attrs, rotation=30, ha="right")
    ax.set_title(f"{dimension.title()} — {metric}")
    ylabel = f"{metric}"
    if as_percent:
        ylabel += " (%)"
    ax.set_ylabel(ylabel)
    ax.legend(title="model")
    ax.margins(x=0.02)
    fig.tight_layout()
    out_path = outdir / f"summary_{dimension}_{metric}.png"
    fig.savefig(out_path, dpi=200)
    plt.close(fig)
    return out_path


def plot_deltas(df_tests: pd.DataFrame, dimension: str, metric: str, outdir: Path):
    data = df_tests[(df_tests["dimension"] == dimension) & (df_tests["metric"] == metric)].copy()
    if data.empty:
        return None

    # Gender: paired deltas per model (no attribute column)
    if dimension == "gender":
        # Expect columns: model, delta_mean, p_value
        models = sorted(data["model"].unique())
        vals = [float(data[data["model"] == m]["delta_mean"].iloc[0]) for m in models]
        ps = [float(data[data["model"] == m]["p_value"].iloc[0]) for m in models]

        fig, ax = plt.subplots(figsize=(max(6, 1.6 * len(models)), 4.5))
        bars = ax.bar(models, vals, color="#4e79a7")
        ax.axhline(0, color="#333", lw=1)
        for b, p in zip(bars, ps):
            star = _sig_stars(p)
            if star:
                ax.text(b.get_x() + b.get_width() / 2, b.get_height(), star, ha="center", va="bottom", fontsize=14)
        ax.set_ylabel(f"delta_mean ({metric})")
        ax.set_title(f"Gender paired delta — {metric}")
        fig.tight_layout()
        out_path = outdir / f"delta_{dimension}_{metric}.png"
        fig.savefig(out_path, dpi=200)
        plt.close(fig)
        return out_path

    # Other dimensions: attribute vs rest per model
    # Expect columns: attribute_value, model, delta, p_value
    attrs = sorted(data["attribute_value"].dropna().unique())
    models = sorted(data["model"].unique())
    x = np.arange(len(attrs))
    n_models = len(models)
    width = 0.8 / max(1, n_models)
    offsets = (np.arange(n_models) - (n_models - 1) / 2.0) * width

    fig, ax = plt.subplots(figsize=(max(8, 1.2 * len(attrs)), 5.5))
    for i, m in enumerate(models):
        sub = data[data["model"] == m].set_index("attribute_value")
        vals = [sub.loc[a, "delta"] if a in sub.index else np.nan for a in attrs]
        bars = ax.bar(x + offsets[i], vals, width=width, label=m)
        # significance stars
        for j, a in enumerate(attrs):
            if a in sub.index:
                p = float(sub.loc[a, "p_value"])
                star = _sig_stars(p)
                if star and not math.isnan(vals[j]):
                    ax.text(x[j] + offsets[i], vals[j], star, ha="center", va="bottom", fontsize=12)

    ax.axhline(0, color="#333", lw=1)
    ax.set_xticks(x)
    ax.set_xticklabels(attrs, rotation=30, ha="right")
    ax.set_ylabel(f"delta (attr − rest) — {metric}")
    ax.set_title(f"{dimension.title()} deltas — {metric}")
    ax.legend(title="model")
    ax.margins(x=0.02)
    fig.tight_layout()
    out_path = outdir / f"delta_{dimension}_{metric}.png"
    fig.savefig(out_path, dpi=200)
    plt.close(fig)
    return out_path


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--summary_csv", default=str(ROOT / "analysis" / "outputs" / "summary_per_attribute.csv"))
    ap.add_argument("--tests_csv",   default=str(ROOT / "analysis" / "outputs" / "tests_all_metrics.csv"))
    ap.add_argument("--metrics", nargs="*", default=["bias_score", "favorability", "refusal"])  # sensible defaults
    ap.add_argument("--outdir", default=str(OUT_DIR))
    args = ap.parse_args()

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    df_sum = pd.read_csv(args.summary_csv)
    df_tests = None
    if Path(args.tests_csv).exists():
        df_tests = pd.read_csv(args.tests_csv)

    dimensions = sorted(df_sum["dimension"].unique())

    made = []
    for dim in dimensions:
        for metric in args.metrics:
            p1 = plot_summary(df_sum, dim, metric, outdir)
            if p1: made.append(p1)
            if df_tests is not None and not df_tests.empty and metric in set(df_tests.get("metric", [])):
                p2 = plot_deltas(df_tests, dim, metric, outdir)
                if p2: made.append(p2)

    print("Wrote:")
    for p in made:
        print(" -", p)


if __name__ == "__main__":
    main()

